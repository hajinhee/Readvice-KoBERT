{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. GPU 사용가능 -> True, GPU 사용불가 -> Flase\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print('1. GPU 사용가능 -> True, GPU 사용불가 -> Flase')\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. GPU 사용중 -> True, GPU 사용안함 -> Flase\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "print('2. GPU 사용중 -> True, GPU 사용안함 -> Flase')\n",
    "print(USE_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. 학습을 진행하는 기기: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if USE_CUDA else 'cpu')\n",
    "print('3. 학습을 진행하는 기기:',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. GPU 이름 체크(cuda:0 에 연결된 그래픽 카드 기준)\n",
      "NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print('4. GPU 이름 체크(cuda:0 에 연결된 그래픽 카드 기준)')\n",
    "print(torch.cuda.get_device_name()) # 'NVIDIA TITAN X (Pascal)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. 사용 가능 GPU 개수 체크\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print('5. 사용 가능 GPU 개수 체크')\n",
    "print(torch.cuda.device_count()) # 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17526994417015622135\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1751030171\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 2180419307828140891\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 416903419\n",
      "]\n",
      "True\n",
      "학습을 진행하는 기기: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "tf.config.list_physical_devices('GPU')\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "print(USE_CUDA)\n",
    "\n",
    "device = torch.device('cuda:0' if USE_CUDA else 'cpu')\n",
    "print('학습을 진행하는 기기:',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kobert\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "#transformers\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU 사용\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. c:\\Users\\jinhee\\project_readvice\\readvice_KoBERT\\kobert\\.cache\\kobert_v1.zip\n",
      "using cached model. c:\\Users\\jinhee\\project_readvice\\readvice_KoBERT\\kobert\\.cache\\kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "#BERT 모델, Vocabulary 불러오기\n",
    "bertmodel, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "chatbot_data = pd.read_excel('../kobert/data/한국어_단발성_대화_데이터셋.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>공포</th>\n",
       "      <th>5468</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18105</th>\n",
       "      <td>맨날오빠생각만해요</td>\n",
       "      <td>슬픔</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35436</th>\n",
       "      <td>지코랑 설현이랑 헤어지게해주세요</td>\n",
       "      <td>혐오</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15876</th>\n",
       "      <td>반드시처벌받게합시다두년들</td>\n",
       "      <td>분노</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11825</th>\n",
       "      <td>역시 여자가 나서니 나라가 망해가네</td>\n",
       "      <td>분노</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37409</th>\n",
       "      <td>이게 다 대통령 잘못 뽑아서 그래</td>\n",
       "      <td>혐오</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3252</th>\n",
       "      <td>한번 할때마다 왼쪽으로 휘고있는걸 느껴요ㄱ</td>\n",
       "      <td>공포</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21654</th>\n",
       "      <td>살도 엄.청 쪄서 뭘입어도 폼이안남 ㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ</td>\n",
       "      <td>슬픔</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16113</th>\n",
       "      <td>한국에 이슬람사원 지으면 누군가가 불질러야 하고 대한민국 소방대원은 2시간후 출동 ...</td>\n",
       "      <td>분노</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34154</th>\n",
       "      <td>국민들 분열 시키지 말고, 민주주의 나라에서 투표로 결정해라... 일주일 지나 달라...</td>\n",
       "      <td>혐오</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35458</th>\n",
       "      <td>친일 사학자들이 판치는 대한민국 현실</td>\n",
       "      <td>혐오</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Sentence Emotion  Unnamed: 2  \\\n",
       "18105                                          맨날오빠생각만해요      슬픔         NaN   \n",
       "35436                                  지코랑 설현이랑 헤어지게해주세요      혐오         NaN   \n",
       "15876                                      반드시처벌받게합시다두년들      분노         NaN   \n",
       "11825                                역시 여자가 나서니 나라가 망해가네      분노         NaN   \n",
       "37409                                 이게 다 대통령 잘못 뽑아서 그래      혐오         NaN   \n",
       "3252                             한번 할때마다 왼쪽으로 휘고있는걸 느껴요ㄱ      공포         NaN   \n",
       "21654                    살도 엄.청 쪄서 뭘입어도 폼이안남 ㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ      슬픔         NaN   \n",
       "16113  한국에 이슬람사원 지으면 누군가가 불질러야 하고 대한민국 소방대원은 2시간후 출동 ...      분노         NaN   \n",
       "34154  국민들 분열 시키지 말고, 민주주의 나라에서 투표로 결정해라... 일주일 지나 달라...      혐오         NaN   \n",
       "35458                               친일 사학자들이 판치는 대한민국 현실      혐오         NaN   \n",
       "\n",
       "       Unnamed: 3  Unnamed: 4   공포  5468  \n",
       "18105         NaN         NaN  NaN   NaN  \n",
       "35436         NaN         NaN  NaN   NaN  \n",
       "15876         NaN         NaN  NaN   NaN  \n",
       "11825         NaN         NaN  NaN   NaN  \n",
       "37409         NaN         NaN  NaN   NaN  \n",
       "3252          NaN         NaN  NaN   NaN  \n",
       "21654         NaN         NaN  NaN   NaN  \n",
       "16113         NaN         NaN  NaN   NaN  \n",
       "34154         NaN         NaN  NaN   NaN  \n",
       "35458         NaN         NaN  NaN   NaN  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_data.sample(n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"공포\"), 'Emotion'] = 0  #공포 => 0\n",
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"놀람\"), 'Emotion'] = 1  #놀람 => 1\n",
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"분노\"), 'Emotion'] = 2  #분노 => 2\n",
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"슬픔\"), 'Emotion'] = 3  #슬픔 => 3\n",
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"중립\"), 'Emotion'] = 4  #중립 => 4\n",
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"행복\"), 'Emotion'] = 5  #행복 => 5\n",
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"혐오\"), 'Emotion'] = 6  #혐오 => 6\n",
    "\n",
    "data_list = []\n",
    "for q, label in zip(chatbot_data['Sentence'], chatbot_data['Emotion'])  :\n",
    "    data = []\n",
    "    data.append(q)\n",
    "    data.append(str(label))\n",
    "\n",
    "    data_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['언니 동생으로 부르는게 맞는 일인가요..??', '0']\n",
      "['기술적으로도 아직도 해체해서 다시 완벽히 돌려놓는게 어려운데 해체를한다고?', '1']\n",
      "['당연히 그렇게 해야지 우리나라도 판매를 중단하라', '2']\n",
      "['그거들은 뒤부터 미치겠어요...', '3']\n",
      "['최악의 상황중 그나마 나은 방법이네. 기분은 잡치겠지만', '4']\n",
      "['  요리하는것이 숙제하는것처럼 힘든저에게 용기나게 해주시고 할수 있을것같은 희망을 주셔서감사합니다!!', '5']\n",
      "['와이프도 그렇고 댓글 다 볼텐데 이휘재 좀 하차 하라고 전해주세요', '6']\n"
     ]
    }
   ],
   "source": [
    "print(data_list[0])\n",
    "print(data_list[6000])\n",
    "print(data_list[12000])\n",
    "print(data_list[18000])\n",
    "print(data_list[24000])\n",
    "print(data_list[30000])\n",
    "print(data_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train & test 데이터로 나누기\n",
    "from sklearn.model_selection import train_test_split\n",
    "                                                         \n",
    "dataset_train, dataset_test = train_test_split(data_list, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28945\n",
      "9649\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_train))\n",
    "print(len(dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters\n",
    "max_len = 64\n",
    "batch_size = 5\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 2\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. c:\\Users\\jinhee\\project_readvice\\readvice_KoBERT\\kobert\\.cache\\kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "#토큰화\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "\n",
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   2, 1189,  517, 6188, 7245, 7063,  517,  463, 3486, 7836, 5966,\n",
       "        1698,  517, 6188, 7245, 7063,  517,  463, 1281, 7870, 1801, 6885,\n",
       "        7088, 5966, 1698, 5837, 5837,  517, 6188, 7245, 6398, 6037, 7063,\n",
       "         517,  463,  517,  463,  517,  364,  517,  364,    3,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1]),\n",
       " array(42),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 5)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=0)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=7,   ##클래스 수 조정##\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "595"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, gc\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\readvice\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2739d2fe3a0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BERT 모델 불러오기\n",
    "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n",
    "\n",
    "#optimizer와 schedule 설정\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "#정확도 측정을 위한 함수 정의\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "    \n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jinhee\\AppData\\Local\\Temp\\ipykernel_18132\\2673822008.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbcdd1720b5e47a38dc420e1aa90c1e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5789 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 2.2211496829986572 train acc 0.0\n",
      "epoch 1 batch id 201 loss 1.906883955001831 train acc 0.1771144278606964\n",
      "epoch 1 batch id 401 loss 1.9127635955810547 train acc 0.2054862842892774\n",
      "epoch 1 batch id 601 loss 1.6356290578842163 train acc 0.2529118136439273\n",
      "epoch 1 batch id 801 loss 1.9122178554534912 train acc 0.27565543071161025\n",
      "epoch 1 batch id 1001 loss 1.8329330682754517 train acc 0.29810189810189736\n",
      "epoch 1 batch id 1201 loss 1.6686519384384155 train acc 0.3095753538717717\n",
      "epoch 1 batch id 1401 loss 2.4543402194976807 train acc 0.3189150606709472\n",
      "epoch 1 batch id 1601 loss 2.3405873775482178 train acc 0.3266708307307907\n",
      "epoch 1 batch id 1801 loss 1.5508021116256714 train acc 0.33714602998334103\n",
      "epoch 1 batch id 2001 loss 1.6600208282470703 train acc 0.3452273863068457\n",
      "epoch 1 batch id 2201 loss 1.2950736284255981 train acc 0.349750113584734\n",
      "epoch 1 batch id 2401 loss 1.3879727125167847 train acc 0.35760099958350705\n",
      "epoch 1 batch id 2601 loss 1.1453475952148438 train acc 0.3652441368704347\n",
      "epoch 1 batch id 2801 loss 1.9445626735687256 train acc 0.3692252766868982\n",
      "epoch 1 batch id 3001 loss 1.2063405513763428 train acc 0.3738087304231934\n",
      "epoch 1 batch id 3201 loss 1.8518108129501343 train acc 0.37825679475164126\n",
      "epoch 1 batch id 3401 loss 1.1716268062591553 train acc 0.38318141723022775\n",
      "epoch 1 batch id 3601 loss 0.9845684170722961 train acc 0.3886142738128316\n",
      "epoch 1 batch id 3801 loss 2.6932809352874756 train acc 0.39358063667456117\n",
      "epoch 1 batch id 4001 loss 1.7808080911636353 train acc 0.39845038740315114\n",
      "epoch 1 batch id 4201 loss 1.6833598613739014 train acc 0.4008569388240916\n",
      "epoch 1 batch id 4401 loss 0.9288597106933594 train acc 0.4041354237673275\n",
      "epoch 1 batch id 4601 loss 1.3655356168746948 train acc 0.40643338404694856\n",
      "epoch 1 batch id 4801 loss 1.6346416473388672 train acc 0.4114142886898581\n",
      "epoch 1 batch id 5001 loss 1.8483257293701172 train acc 0.4155168966206773\n",
      "epoch 1 batch id 5201 loss 1.4104492664337158 train acc 0.4183810805614315\n",
      "epoch 1 batch id 5401 loss 1.7263543605804443 train acc 0.4219218663210524\n",
      "epoch 1 batch id 5601 loss 1.3824607133865356 train acc 0.4232815568648456\n",
      "epoch 1 train acc 0.4254620832613576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jinhee\\AppData\\Local\\Temp\\ipykernel_18132\\2673822008.py:23: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c2e8b2a96d3488ab3a3dc4ffa9c91d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1930 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.514507772020726\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d1db44eb44a4b90abfff4d5430afedd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5789 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 1.1025818586349487 train acc 0.6\n",
      "epoch 2 batch id 201 loss 1.7564709186553955 train acc 0.518407960199005\n",
      "epoch 2 batch id 401 loss 1.7622041702270508 train acc 0.5147132169576063\n",
      "epoch 2 batch id 601 loss 0.9127349853515625 train acc 0.5194675540765394\n",
      "epoch 2 batch id 801 loss 1.568588376045227 train acc 0.5238451935081156\n",
      "epoch 2 batch id 1001 loss 1.376814842224121 train acc 0.5270729270729277\n",
      "epoch 2 batch id 1201 loss 1.632665991783142 train acc 0.5292256452955871\n",
      "epoch 2 batch id 1401 loss 1.9804471731185913 train acc 0.5266238401142042\n",
      "epoch 2 batch id 1601 loss 1.3607540130615234 train acc 0.5324172392254832\n",
      "epoch 2 batch id 1801 loss 1.2397572994232178 train acc 0.5360355358134357\n",
      "epoch 2 batch id 2001 loss 0.965359091758728 train acc 0.5437281359320316\n",
      "epoch 2 batch id 2201 loss 0.974568247795105 train acc 0.5440254429804601\n",
      "epoch 2 batch id 2401 loss 0.6465607285499573 train acc 0.5501041232819617\n",
      "epoch 2 batch id 2601 loss 0.2950831651687622 train acc 0.5534794309880766\n",
      "epoch 2 batch id 2801 loss 1.8933985233306885 train acc 0.5556586933238069\n",
      "epoch 2 batch id 3001 loss 0.8815608024597168 train acc 0.5566811062978945\n",
      "epoch 2 batch id 3201 loss 1.041739583015442 train acc 0.5606997813183309\n",
      "epoch 2 batch id 3401 loss 1.1840431690216064 train acc 0.5630108791531827\n",
      "epoch 2 batch id 3601 loss 0.593216061592102 train acc 0.5656762010552542\n",
      "epoch 2 batch id 3801 loss 1.3324291706085205 train acc 0.5694817153380626\n",
      "epoch 2 batch id 4001 loss 1.0048606395721436 train acc 0.57315671082229\n",
      "epoch 2 batch id 4201 loss 0.8939264416694641 train acc 0.5742442275648623\n",
      "epoch 2 batch id 4401 loss 1.5730043649673462 train acc 0.5763235628266283\n",
      "epoch 2 batch id 4601 loss 0.5789871215820312 train acc 0.5779613127580954\n",
      "epoch 2 batch id 4801 loss 1.0415959358215332 train acc 0.5809622995209338\n",
      "epoch 2 batch id 5001 loss 1.4847866296768188 train acc 0.5834833033393343\n",
      "epoch 2 batch id 5201 loss 1.4017372131347656 train acc 0.5856566044991381\n",
      "epoch 2 batch id 5401 loss 1.431909203529358 train acc 0.588076282169973\n",
      "epoch 2 batch id 5601 loss 1.0773587226867676 train acc 0.5891447955722244\n",
      "epoch 2 train acc 0.59074106063224\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead4a24705e94a8a8141512775c813e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1930 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.5471502590673564\n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('readvice')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f82e657652c1f559b98fb141e76bcce2ec0f3958c3a6000b4409466ee456e5f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
